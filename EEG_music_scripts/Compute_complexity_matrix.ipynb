{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne # Here we import mne, the package that will contain most of the function that we will use today.\n",
    "from mne.datasets.brainstorm import bst_raw # It is possible to import functions individually. This is helpful since it \n",
    "                                            # saves time, memory, and makes the calls to the function easier.\n",
    "from mne.preprocessing import ICA, create_eog_epochs, create_ecg_epochs\n",
    "%matplotlib notebook\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "# ^ This last line lets us use the interactive plots in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTICIPANTS = ['01', '04', '05', '06', '07', '09', '11', '12', '13', '14']\n",
    "PARTICIPANTS_DROP_CH = ['09', '11', '12', '13', '14']\n",
    "event_id_names = [\n",
    "            'S1_p', 'S1_i',\n",
    "            'S2_p', \n",
    "            'S2_i', \n",
    "            'S3_p', 'S3_i',\n",
    "            'S4_p', 'S4_i',\n",
    "            'S11_p', 'S11_i',\n",
    "            'S12_p', 'S12_i', \n",
    "            'S13_p', 'S13_i',\n",
    "            'S14_p', 'S14_i',\n",
    "            'S21_p', 'S21_i',\n",
    "            'S22_p', 'S22_i', \n",
    "            'S23_p', 'S23_i',\n",
    "            'S24_p', 'S24_i']\n",
    "PERCEPT = [\n",
    "            'S1_p', \n",
    "            'S2_p', \n",
    "            'S3_p',\n",
    "            'S4_p', \n",
    "            'S11_p',\n",
    "            'S12_p', \n",
    "            'S13_p',\n",
    "            'S14_p',\n",
    "            'S21_p',\n",
    "            'S22_p', \n",
    "            'S23_p',\n",
    "            'S24_p',]\n",
    "IMAGINED = [\n",
    "            'S1_i', \n",
    "            'S2_i', \n",
    "            'S3_i',\n",
    "            'S4_i', \n",
    "            'S11_i',\n",
    "            'S12_i', \n",
    "            'S13_i',\n",
    "            'S14_i',\n",
    "            'S21_i',\n",
    "            'S22_i', \n",
    "            'S23_i',\n",
    "            'S24_i',]\n",
    "\n",
    "event_id_cond_names = ['Percept', 'Imagine_cued']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond01.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond01.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 64) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond04.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond04.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 64) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond05.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond05.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 64) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond06.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond06.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 64) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond07.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond07.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 64) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond09.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond09.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 66) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond11.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond11.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 66) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond12.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond12.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 66) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond13.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond13.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 66) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond14.fif.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-86b80cbc4d90>:8: RuntimeWarning: This filename (C:\\Users\\Dell\\Jupyter\\MNE_python\\epoched_data_cond14.fif.gz) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoched_data[p] = mne.read_epochs(fnames, preload=True);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 66) active\n",
      "    Found the data of interest:\n",
      "        t =    -500.00 ...    6800.78 ms\n",
      "        0 CTF compensation matrices available\n",
      "120 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    }
   ],
   "source": [
    "#Load epoched data\n",
    "data_path = r'C:\\Users\\Dell\\Jupyter\\MNE_python'\n",
    "epoched_data = {}\n",
    "for p in PARTICIPANTS:\n",
    "    fnames = (data_path + '\\\\' + 'epoched_data_cond' + p +'.fif.gz'); #Load 'epoched_data_cond' if you want to compare \n",
    "                                                                      #perception vs. imagination\n",
    "                                                                      #Load 'epoched_data' if you want to compare songs as well\n",
    "    epoched_data[p] = mne.read_epochs(fnames, preload=True);\n",
    "    epoched_data[p] = epoched_data[p].apply_baseline((-0.5, -0.1))\n",
    "#Drop extra channels for certain participants\n",
    "for p in PARTICIPANTS_DROP_CH:\n",
    "    epoched_data[p].drop_channels('EXG6')\n",
    "    epoched_data[p].drop_channels('EXG5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = r'C:\\Users\\Dell\\Jupyter\\BrainHackSchool2019_AB'\n",
    "\n",
    "def compute_complexity(epoched_data):\n",
    "    start = time.time()\n",
    "    #for p in PARTICIPANTS:\n",
    "    total_cp = []\n",
    "    for p in PARTICIPANTS:  \n",
    "        conditions_cp = []\n",
    "        for c in event_id_cond_names:\n",
    "            meta_cp = []\n",
    "            complex_data = epoched_data[p][c].get_data()\n",
    "            for i in range(60): #data.shape(0):\n",
    "                cp = []\n",
    "                for j in range(64):\n",
    "                    cp.append(complexity(complex_data[i,j,:], sampling_rate=epoched_data['01'].info['sfreq'],\n",
    "                                        shannon=False, sampen=False, multiscale=False, spectral=True, svd=True, \n",
    "                                        correlation=False, higushi=False, petrosian=False, fisher=False, hurst=False, \n",
    "                                         dfa=False, lyap_r=False, lyap_e=False, bands=[1, 3]))\n",
    "                meta_cp.append(cp)\n",
    "            conditions_cp.append(meta_cp)\n",
    "        total_cp.append(conditions_cp)\n",
    "            #sio.savemat('{}_{}'.format(p, c), {'complexity': meta_cp})\n",
    "    stop = time.time()\n",
    "    print(stop - start)\n",
    "    total_cp = np.array(total_cp)\n",
    "    return(total_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('spectral_theta', {'complexity': total_cp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.384005308151245\n"
     ]
    }
   ],
   "source": [
    "total_cp = compute_complexity(epoched_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\n",
    "def hurst(X):\n",
    "    \"\"\" Compute the Hurst exponent of X. If the output H=0.5,the behavior\n",
    "    of the time-series is similar to random walk. If H<0.5, the time-series\n",
    "    cover less \"distance\" than a random walk, vice verse.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        list\n",
    "        a time series\n",
    "    Returns\n",
    "    -------\n",
    "    H\n",
    "        float\n",
    "        Hurst exponent\n",
    "    Notes\n",
    "    --------\n",
    "    Author of this function is Xin Liu\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pyeeg\n",
    "    >>> from numpy.random import randn\n",
    "    >>> a = randn(4096)\n",
    "    >>> pyeeg.hurst(a)\n",
    "    0.5057444\n",
    "    \"\"\"\n",
    "    X = numpy.array(X)\n",
    "    N = X.size\n",
    "    T = numpy.arange(1, N + 1)\n",
    "    Y = numpy.cumsum(X)\n",
    "    Ave_T = Y / T\n",
    "\n",
    "    S_T = numpy.zeros(N)\n",
    "    R_T = numpy.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        S_T[i] = numpy.std(X[:i + 1])\n",
    "        X_T = Y - T * Ave_T[i]\n",
    "        R_T[i] = numpy.ptp(X_T[:i + 1])\n",
    "\n",
    "    R_S = R_T / S_T\n",
    "    R_S = numpy.log(R_S)[1:]\n",
    "    n = numpy.log(T)[1:]\n",
    "    A = numpy.column_stack((n, numpy.ones(n.size)))\n",
    "    [m, c] = numpy.linalg.lstsq(A, R_S)[0]\n",
    "    H = m\n",
    "    return H\n",
    "\n",
    "\n",
    "import nolds\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "def complexity(signal, sampling_rate=1000, shannon=False, sampen=False, multiscale=False, spectral=False, svd=False, correlation=False, higushi=False, petrosian=True, fisher=False, hurst=False, dfa=False, lyap_r=False, lyap_e=False, emb_dim=2, tolerance=\"default\", k_max=8, bands=None, tau=1):\n",
    "    \"\"\"\n",
    "    Computes several chaos/complexity indices of a signal (including entropy, fractal dimensions, Hurst and Lyapunov exponent etc.).\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : list or array\n",
    "        List or array of values.\n",
    "    sampling_rate : int\n",
    "        Sampling rate (samples/second).\n",
    "    shannon : bool\n",
    "        Computes Shannon entropy.\n",
    "    sampen : bool\n",
    "        Computes approximate sample entropy (sampen) using Chebychev and Euclidean distances.\n",
    "    multiscale : bool\n",
    "        Computes multiscale entropy (MSE). Note that it uses the 'euclidean' distance.\n",
    "    spectral : bool\n",
    "        Computes Spectral Entropy.\n",
    "    svd : bool\n",
    "        Computes the Singular Value Decomposition (SVD) entropy.\n",
    "    correlation : bool\n",
    "        Computes the fractal (correlation) dimension.\n",
    "    higushi : bool\n",
    "        Computes the Higushi fractal dimension.\n",
    "    petrosian : bool\n",
    "        Computes the Petrosian fractal dimension.\n",
    "    fisher : bool\n",
    "        Computes the Fisher Information.\n",
    "    hurst : bool\n",
    "        Computes the Hurst exponent.\n",
    "    dfa : bool\n",
    "        Computes DFA.\n",
    "    lyap_r : bool\n",
    "        Computes Positive Lyapunov exponents (Rosenstein et al. (1993) method).\n",
    "    lyap_e : bool\n",
    "        Computes Positive Lyapunov exponents (Eckmann et al. (1986) method).\n",
    "    emb_dim : int\n",
    "        The embedding dimension (*m*, the length of vectors to compare). Used in sampen, fisher, svd and fractal_dim.\n",
    "    tolerance : float\n",
    "        Distance *r* threshold for two template vectors to be considered equal. Default is 0.2*std(signal). Used in sampen and fractal_dim.\n",
    "    k_max : int\n",
    "        The maximal value of k used for Higushi fractal dimension. The point at which the FD plateaus is considered a saturation point and that kmax value should be selected (Gómez, 2009). Some studies use a value of 8 or 16 for ECG signal and other 48 for MEG.\n",
    "    bands : int\n",
    "        Used for spectral density. A list of numbers delimiting the bins of the frequency bands. If None the entropy is computed over the whole range of the DFT (from 0 to `f_s/2`).\n",
    "    tau : int\n",
    "        The delay. Used for fisher, svd, lyap_e and lyap_r.\n",
    "    Returns\n",
    "    ----------\n",
    "    complexity : dict\n",
    "        Dict containing values for each indices.\n",
    "    Example\n",
    "    ----------\n",
    "    >>> import neurokit as nk\n",
    "    >>> import numpy as np\n",
    "    >>>\n",
    "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
    "    >>> complexity = nk.complexity(signal)\n",
    "    Notes\n",
    "    ----------\n",
    "    *Details*\n",
    "    - **Entropy**: Entropy is a measure of unpredictability of the state, or equivalently, of its average information content.\n",
    "      - *Shannon entropy*: Shannon entropy was introduced by Claude E. Shannon in his 1948 paper \"A Mathematical Theory of Communication\". Shannon entropy provides an absolute limit on the best possible average length of lossless encoding or compression of an information source.\n",
    "      - *Sample entropy (sampen)*: Measures the complexity of a time-series, based on approximate entropy. The sample entropy of a time series is defined as the negative natural logarithm of the conditional probability that two sequences similar for emb_dim points remain similar at the next point, excluding self-matches. A lower value for the sample entropy therefore corresponds to a higher probability indicating more self-similarity.\n",
    "      - *Multiscale entropy*: Multiscale entropy (MSE) analysis is a new method of measuring the complexity of finite length time series.\n",
    "      - *SVD Entropy*: Indicator of how many vectors are needed for an adequate explanation of the data set. Measures feature-richness in the sense that the higher the entropy of the set of SVD weights, the more orthogonal vectors are required to adequately explain it.\n",
    "    - **fractal dimension**: The term *fractal* was first introduced by Mandelbrot in 1983. A fractal is a set of points that when looked at smaller scales, resembles the whole set. The concept of fractak dimension (FD) originates from fractal geometry. In traditional geometry, the topological or Euclidean dimension of an object is known as the number of directions each differential of the object occupies in space. This definition of dimension works well for geometrical objects whose level of detail, complexity or *space-filling* is the same. However, when considering two fractals of the same topological dimension, their level of *space-filling* is different, and that information is not given by the topological dimension. The FD emerges to provide a measure of how much space an object occupies between Euclidean dimensions. The FD of a waveform represents a powerful tool for transient detection. This feature has been used in the analysis of ECG and EEG to identify and distinguish specific states of physiologic function. Many algorithms are available to determine the FD of the waveform (Acharya, 2005).\n",
    "      - *Correlation*: A measure of the fractal (or correlation) dimension of a time series which is also related to complexity. The correlation dimension is a characteristic measure that can be used to describe the geometry of chaotic attractors. It is defined using the correlation sum C(r) which is the fraction of pairs of points X_i in the phase space whose distance is smaller than r.\n",
    "      - *Higushi*: Higuchi proposed in 1988 an efficient algorithm for measuring the FD of discrete time sequences. As the reconstruction of the attractor phase space is not necessary, this algorithm is simpler and faster than D2 and other classical measures derived from chaos theory. FD can be used to quantify the complexity and self-similarity of a signal. HFD has already been used to analyse the complexity of brain recordings and other biological signals.\n",
    "      - *Petrosian Fractal Dimension*: Provide a fast computation of the FD of a signal by translating the series into a binary sequence.\n",
    "    - **Other**:\n",
    "      - *Fisher Information*:  A way of measuring the amount of information that an observable random variable X carries about an unknown parameter θ of a distribution that models X. Formally, it is the variance of the score, or the expected value of the observed information.\n",
    "      - *Hurst*: The Hurst exponent is a measure of the \"long-term memory\" of a time series. It can be used to determine whether the time series is more, less, or equally likely to increase if it has increased in previous steps. This property makes the Hurst exponent especially interesting for the analysis of stock data.\n",
    "      - *DFA*: DFA measures the Hurst parameter H, which is very similar to the Hurst exponent. The main difference is that DFA can be used for non-stationary processes (whose mean and/or variance change over time).\n",
    "      - *Lyap*: Positive Lyapunov exponents indicate chaos and unpredictability. Provides the algorithm of Rosenstein et al. (1993) to estimate the largest Lyapunov exponent and the algorithm of Eckmann et al. (1986) to estimate the whole spectrum of Lyapunov exponents.\n",
    "    *Authors*\n",
    "    - Dominique Makowski (https://github.com/DominiqueMakowski)\n",
    "    - Christopher Schölzel (https://github.com/CSchoel)\n",
    "    - tjugo (https://github.com/nikdon)\n",
    "    - Quentin Geissmann (https://github.com/qgeissmann)\n",
    "    *Dependencies*\n",
    "    - nolds\n",
    "    - numpy\n",
    "    *See Also*\n",
    "    - nolds package: https://github.com/CSchoel/nolds\n",
    "    - pyEntropy package: https://github.com/nikdon/pyEntropy\n",
    "    - pyrem package: https://github.com/gilestrolab/pyrem\n",
    "    References\n",
    "    -----------\n",
    "    - Accardo, A., Affinito, M., Carrozzi, M., & Bouquet, F. (1997). Use of the fractal dimension for the analysis of electroencephalographic time series. Biological cybernetics, 77(5), 339-350.\n",
    "    - Pierzchalski, M. Application of Higuchi Fractal Dimension in Analysis of Heart Rate Variability with Artificial and Natural Noise. Recent Advances in Systems Science.\n",
    "    - Acharya, R., Bhat, P. S., Kannathal, N., Rao, A., & Lim, C. M. (2005). Analysis of cardiac health using fractal dimension and wavelet transformation. ITBM-RBM, 26(2), 133-139.\n",
    "    - Richman, J. S., & Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n",
    "    - Costa, M., Goldberger, A. L., & Peng, C. K. (2005). Multiscale entropy analysis of biological signals. Physical review E, 71(2), 021906.\n",
    "    \"\"\"\n",
    "\n",
    "    if tolerance == \"default\":\n",
    "        tolerance = 0.2*np.std(signal)\n",
    "\n",
    "    # Initialize results storing\n",
    "    complexity = {}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "    # Shannon\n",
    "    if shannon is True:\n",
    "        try:\n",
    "            complexity[\"Entropy_Shannon\"] = complexity_entropy_shannon(signal)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute Shannon entropy.\")\n",
    "            complexity[\"Entropy_Shannon\"] = np.nan\n",
    "\n",
    "    # Sampen\n",
    "    if sampen is True:\n",
    "        try:\n",
    "            complexity[\"Entropy_Sample\"] = nolds.sampen(signal, emb_dim, tolerance, dist=\"chebychev\", debug_plot=False, plot_file=None)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute sample entropy (sampen).\")\n",
    "            complexity[\"Entropy_Sample\"] = np.nan\n",
    "\n",
    "\n",
    "    # multiscale\n",
    "    if multiscale is True:\n",
    "        try:\n",
    "            complexity[\"Entropy_Multiscale\"] = complexity_entropy_multiscale(signal, emb_dim, tolerance)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute Multiscale Entropy (MSE).\")\n",
    "            complexity[\"Entropy_Multiscale\"] = np.nan\n",
    "\n",
    "    # spectral\n",
    "    if spectral is True:\n",
    "        try:\n",
    "            complexity[\"Entropy_Spectral\"] = complexity_entropy_spectral(signal, sampling_rate=sampling_rate, bands=bands)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute Spectral Entropy.\")\n",
    "            complexity[\"Entropy_Spectral\"] = np.nan\n",
    "\n",
    "    # SVD\n",
    "    if svd is True:\n",
    "        try:\n",
    "            complexity[\"Entropy_SVD\"] = complexity_entropy_svd(signal, tau=tau, emb_dim=emb_dim)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute SVD Entropy.\")\n",
    "            complexity[\"Entropy_SVD\"] = np.nan\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "    # fractal_dim\n",
    "    if correlation is True:\n",
    "        try:\n",
    "            complexity[\"Fractal_Dimension_Correlation\"] = nolds.corr_dim(signal, emb_dim, rvals=None, fit=\"RANSAC\", debug_plot=False, plot_file=None)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute fractal_dim.\")\n",
    "            complexity[\"Fractal_Dimension_Correlation\"] = np.nan\n",
    "\n",
    "    # higushi\n",
    "    if higushi is True:\n",
    "        try:\n",
    "            complexity[\"Fractal_Dimension_Higushi\"] = complexity_fd_higushi(signal, k_max)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute higushi.\")\n",
    "            complexity[\"Fractal_Dimension_Higushi\"] = np.nan\n",
    "\n",
    "    # petrosian\n",
    "    if petrosian is True:\n",
    "        try:\n",
    "            complexity[\"Fractal_Dimension_Petrosian\"] = complexity_fd_petrosian(signal)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute petrosian.\")\n",
    "            complexity[\"Fractal_Dimension_Petrosian\"] = np.nan\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "    # Fisher\n",
    "    if fisher is True:\n",
    "        try:\n",
    "            complexity[\"Fisher_Information\"] = complexity_fisher_info(signal, tau=tau, emb_dim=emb_dim)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute Fisher Information.\")\n",
    "            complexity[\"Fisher_Information\"] = np.nan\n",
    "\n",
    "\n",
    "    # Hurst\n",
    "    if hurst is True:\n",
    "        try:\n",
    "            complexity[\"Hurst\"] = nolds.hurst_rs(signal, nvals=None, fit=\"RANSAC\", debug_plot=False, plot_file=None)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute hurst.\")\n",
    "            complexity[\"Hurst\"] = np.nan\n",
    "\n",
    "    # DFA\n",
    "    if dfa is True:\n",
    "        try:\n",
    "            complexity[\"DFA\"] = nolds.dfa(signal, nvals=None, overlap=True, order=1, fit_trend=\"poly\", fit_exp=\"RANSAC\", debug_plot=False, plot_file=None)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute dfa.\")\n",
    "            complexity[\"DFA\"] = np.nan\n",
    "\n",
    "    # Lyap_r\n",
    "    if lyap_r is True:\n",
    "        try:\n",
    "            complexity[\"Lyapunov_R\"] = nolds.lyap_r(signal, emb_dim=10, lag=None, min_tsep=None, tau=tau, min_vectors=20, trajectory_len=20, fit=\"RANSAC\", debug_plot=False, plot_file=None)\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute lyap_r.\")\n",
    "            complexity[\"Lyapunov_R\"] = np.nan\n",
    "\n",
    "    # Lyap_e\n",
    "    if lyap_e is True:\n",
    "        try:\n",
    "            result = nolds.lyap_e(signal, emb_dim=10, matrix_dim=4, min_nb=None, min_tsep=0, tau=tau, debug_plot=False, plot_file=None)\n",
    "            for i, value in enumerate(result):\n",
    "                complexity[\"Lyapunov_E_\" + str(i)] = value\n",
    "        except:\n",
    "            print(\"NeuroKit warning: complexity(): Failed to compute lyap_e.\")\n",
    "            complexity[\"Lyapunov_E\"] = np.nan\n",
    "\n",
    "    return(complexity)\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "def complexity_entropy_shannon(signal):\n",
    "    \"\"\"\n",
    "    Computes the shannon entropy. Copied from the `pyEntropy <https://github.com/nikdon/pyEntropy>`_ repo by tjugo.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : list or array\n",
    "        List or array of values.\n",
    "    Returns\n",
    "    ----------\n",
    "    shannon_entropy : float\n",
    "        The Shannon Entropy as float value.\n",
    "    Example\n",
    "    ----------\n",
    "    >>> import neurokit as nk\n",
    "    >>>\n",
    "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
    "    >>> shannon_entropy = nk.complexity_entropy_shannon(signal)\n",
    "    Notes\n",
    "    ----------\n",
    "    *Details*\n",
    "    - **shannon entropy**: Entropy is a measure of unpredictability of the state, or equivalently, of its average information content.\n",
    "    *Authors*\n",
    "    - tjugo (https://github.com/nikdon)\n",
    "    *Dependencies*\n",
    "    - numpy\n",
    "    *See Also*\n",
    "    - pyEntropy package: https://github.com/nikdon/pyEntropy\n",
    "    References\n",
    "    -----------\n",
    "    - None\n",
    "    \"\"\"\n",
    "     # Check if string\n",
    "    if not isinstance(signal, str):\n",
    "        signal = list(signal)\n",
    "\n",
    "    signal = np.array(signal)\n",
    "\n",
    "\n",
    "    # Create a frequency data\n",
    "    data_set = list(set(signal))\n",
    "    freq_list = []\n",
    "    for entry in data_set:\n",
    "        counter = 0.\n",
    "        for i in signal:\n",
    "            if i == entry:\n",
    "                counter += 1\n",
    "        freq_list.append(float(counter) / len(signal))\n",
    "\n",
    "    # Shannon entropy\n",
    "    shannon_entropy = 0.0\n",
    "    for freq in freq_list:\n",
    "        shannon_entropy += freq * np.log2(freq)\n",
    "    shannon_entropy = -shannon_entropy\n",
    "\n",
    "    return(shannon_entropy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "def complexity_entropy_multiscale(signal, max_scale_factor=20, m=2, r=\"default\"):\n",
    "    \"\"\"\n",
    "    Computes the Multiscale Entropy. Uses sample entropy with 'chebychev' distance.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : list or array\n",
    "        List or array of values.\n",
    "    max_scale_factor: int\n",
    "        Max scale factor (*tau*). The max length of coarse-grained time series analyzed. Will analyze scales for all integers from 1:max_scale_factor.\n",
    "        See Costa (2005).\n",
    "    m : int\n",
    "        The embedding dimension (*m*, the length of vectors to compare).\n",
    "    r : float\n",
    "        Similarity factor *r*. Distance threshold for two template vectors to be considered equal. Default is 0.15*std(signal).\n",
    "    Returns\n",
    "    ----------\n",
    "    mse: dict\n",
    "        A dict containing \"MSE_Parameters\" (a dict with the actual max_scale_factor, m and r), \"MSE_Values\" (an array with the sample entropy for each scale_factor up to the max_scale_factor), \"MSE_AUC\" (A float: The area under the MSE_Values curve. A point-estimate of mse) and \"MSE_Sum\" (A float: The sum of MSE_Values curve. Another point-estimate of mse; Norris, 2008).\n",
    "    Example\n",
    "    ----------\n",
    "    >>> import neurokit as nk\n",
    "    >>>\n",
    "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
    "    >>> mse = nk.complexity_entropy_multiscale(signal)\n",
    "    >>> mse_values = mse[\"MSE_Values\"]\n",
    "    Notes\n",
    "    ----------\n",
    "    *Details*\n",
    "    - **multiscale entropy**: Entropy is a measure of unpredictability of the state, or equivalently,\n",
    "    of its average information content. Multiscale entropy (MSE) analysis is a new method of measuring\n",
    "    the complexity of coarse grained versions of the original data, where coarse graining is at all\n",
    "    scale factors from 1:max_scale_factor.\n",
    "    *Authors*\n",
    "    - tjugo (https://github.com/nikdon)\n",
    "    - Dominique Makowski (https://github.com/DominiqueMakowski)\n",
    "    - Anthony Gatti (https://github.com/gattia)\n",
    "    *Dependencies*\n",
    "    - numpy\n",
    "    - nolds\n",
    "    *See Also*\n",
    "    - pyEntropy package: https://github.com/nikdon/pyEntropy\n",
    "    References\n",
    "    -----------\n",
    "    - Richman, J. S., & Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy\n",
    "        and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n",
    "    - Costa, M., Goldberger, A. L., & Peng, C. K. (2005). Multiscale entropy analysis of biological signals.\n",
    "        Physical review E, 71(2), 021906.\n",
    "    - Gow, B. J., Peng, C. K., Wayne, P. M., & Ahn, A. C. (2015). Multiscale entropy analysis of center-of-pressure\n",
    "        dynamics in human postural control: methodological considerations. Entropy, 17(12), 7926-7947.\n",
    "    - Norris, P. R., Anderson, S. M., Jenkins, J. M., Williams, A. E., & Morris Jr, J. A. (2008).\n",
    "        Heart rate multiscale entropy at three hours predicts hospital mortality in 3,154 trauma patients. Shock, 30(1), 17-22.\n",
    "    \"\"\"\n",
    "    if r == \"default\":\n",
    "        r = 0.15*np.std(signal)\n",
    "\n",
    "    n = len(signal)\n",
    "    per_scale_entropy_values = np.zeros(max_scale_factor)\n",
    "\n",
    "    # Compute SampEn for all scale factors\n",
    "    for i in range(max_scale_factor):\n",
    "\n",
    "        b = int(np.fix(n / (i + 1)))\n",
    "        temp_ts = [0] * int(b)\n",
    "\n",
    "        for j in range(b):\n",
    "            num = sum(signal[j * (i + 1): (j + 1) * (i + 1)])\n",
    "            den = i + 1\n",
    "            temp_ts[j] = float(num) / float(den)\n",
    "\n",
    "        se = nolds.sampen(temp_ts, m, r, nolds.measures.rowwise_chebyshev, debug_plot=False, plot_file=None)\n",
    "\n",
    "        if np.isinf(se):\n",
    "            print(\"NeuroKit warning: complexity_entropy_multiscale(): Signal might be to short to compute SampEn for scale factors > \" + str(i) + \". Setting max_scale_factor to \" + str(i) + \".\")\n",
    "            max_scale_factor = i\n",
    "            break\n",
    "        else:\n",
    "            per_scale_entropy_values[i] = se\n",
    "\n",
    "    all_entropy_values = per_scale_entropy_values[0:max_scale_factor]\n",
    "\n",
    "    # Compute final indices\n",
    "    parameters = {\"max_scale_factor\": max_scale_factor,\n",
    "                  \"r\": r,\n",
    "                  \"m\": m}\n",
    "\n",
    "    mse = {\"MSE_Parameters\": parameters,\n",
    "           \"MSE_Values\" : all_entropy_values,\n",
    "           \"MSE_AUC\": np.trapz(all_entropy_values),\n",
    "           \"MSE_Sum\": np.sum(all_entropy_values)}\n",
    "\n",
    "\n",
    "    return (mse)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "def complexity_fd_higushi(signal, k_max):\n",
    "    \"\"\"\n",
    "    Computes Higuchi Fractal Dimension of a signal. Based on the `pyrem <https://github.com/gilestrolab/pyrem>`_ repo by Quentin Geissmann.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : list or array\n",
    "        List or array of values.\n",
    "    k_max : int\n",
    "        The maximal value of k. The point at which the FD plateaus is considered a saturation point and that kmax value should be selected (Gómez, 2009). Some studies use a value of 8 or 16 for ECG signal and other 48 for MEG.\n",
    "    Returns\n",
    "    ----------\n",
    "    fd_higushi : float\n",
    "        The Higushi Fractal Dimension as float value.\n",
    "    Example\n",
    "    ----------\n",
    "    >>> import neurokit as nk\n",
    "    >>>\n",
    "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
    "    >>> fd_higushi = nk.complexity_fd_higushi(signal, 8)\n",
    "    Notes\n",
    "    ----------\n",
    "    *Details*\n",
    "    - **Higushi Fractal Dimension**: Higuchi proposed in 1988 an efficient algorithm for measuring the FD of discrete time sequences. As the reconstruction of the attractor phase space is not necessary, this algorithm is simpler and faster than D2 and other classical measures derived from chaos theory. FD can be used to quantify the complexity and self-similarity of a signal. HFD has already been used to analyse the complexity of brain recordings and other biological signals.\n",
    "    *Authors*\n",
    "    - Quentin Geissmann (https://github.com/qgeissmann)\n",
    "    *Dependencies*\n",
    "    - numpy\n",
    "    *See Also*\n",
    "    - pyrem package: https://github.com/gilestrolab/pyrem\n",
    "    References\n",
    "    -----------\n",
    "    - Accardo, A., Affinito, M., Carrozzi, M., & Bouquet, F. (1997). Use of the fractal dimension for the analysis of electroencephalographic time series. Biological cybernetics, 77(5), 339-350.\n",
    "    - Gómez, C., Mediavilla, Á., Hornero, R., Abásolo, D., & Fernández, A. (2009). Use of the Higuchi's fractal dimension for the analysis of MEG recordings from Alzheimer's disease patients. Medical engineering & physics, 31(3), 306-313.\n",
    "    \"\"\"\n",
    "    signal = np.array(signal)\n",
    "    L = []\n",
    "    x = []\n",
    "    N = signal.size\n",
    "\n",
    "    km_idxs = np.triu_indices(k_max - 1)\n",
    "    km_idxs = k_max - np.flipud(np.column_stack(km_idxs)) -1\n",
    "    km_idxs[:,1] -= 1\n",
    "\n",
    "\n",
    "    for k in range(1, k_max):\n",
    "        Lk = 0\n",
    "        for m in range(0, k):\n",
    "            #we pregenerate all idxs\n",
    "            idxs = np.arange(1,int(np.floor((N-m)/k)))\n",
    "\n",
    "            Lmk = np.sum(np.abs(signal[m+idxs*k] - signal[m+k*(idxs-1)]))\n",
    "            Lmk = (Lmk*(N - 1)/(((N - m)/ k)* k)) / k\n",
    "            Lk += Lmk\n",
    "\n",
    "        if Lk != 0:\n",
    "            L.append(np.log(Lk/(m+1)))\n",
    "            x.append([np.log(1.0/ k), 1])\n",
    "\n",
    "    (p, r1, r2, s)=np.linalg.lstsq(x, L)\n",
    "    fd_higushi = p[0]\n",
    "    return (fd_higushi)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "def complexity_entropy_spectral(signal, sampling_rate, bands=None):\n",
    "    \"\"\"\n",
    "    Computes Spectral Entropy of a signal. Based on the `pyrem <https://github.com/gilestrolab/pyrem>`_ repo by Quentin Geissmann. The power spectrum is computed through fft. Then, it is normalised and assimilated to a probability density function.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : list or array\n",
    "        List or array of values.\n",
    "    sampling_rate : int\n",
    "        Sampling rate (samples/second).\n",
    "    bands : list or array\n",
    "        A list of numbers delimiting the bins of the frequency bands. If None the entropy is computed over the whole range of the DFT (from 0 to `f_s/2`).\n",
    "    Returns\n",
    "    ----------\n",
    "    spectral_entropy : float\n",
    "        The spectral entropy as float value.\n",
    "    Example\n",
    "    ----------\n",
    "    >>> import neurokit as nk\n",
    "    >>>\n",
    "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
    "    >>> spectral_entropy = nk.complexity_entropy_spectral(signal, 1000)\n",
    "    Notes\n",
    "    ----------\n",
    "    *Details*\n",
    "    - **Spectral Entropy**: Entropy for different frequency bands.\n",
    "    *Authors*\n",
    "    - Quentin Geissmann (https://github.com/qgeissmann)\n",
    "    *Dependencies*\n",
    "    - numpy\n",
    "    *See Also*\n",
    "    - pyrem package: https://github.com/gilestrolab/pyrem\n",
    "    \"\"\"\n",
    "\n",
    "    psd = np.abs(np.fft.rfft(signal))**2\n",
    "    psd /= np.sum(psd) # psd as a pdf (normalised to one)\n",
    "\n",
    "    if bands is None:\n",
    "        power_per_band= psd[psd>0]\n",
    "    else:\n",
    "        freqs = np.fft.rfftfreq(signal.size, 1/float(sampling_rate))\n",
    "        bands = np.asarray(bands)\n",
    "\n",
    "        freq_limits_low = np.concatenate([[0.0],bands])\n",
    "        freq_limits_up = np.concatenate([bands, [np.Inf]])\n",
    "\n",
    "        power_per_band = [np.sum(psd[np.bitwise_and(freqs >= low, freqs<up)])\n",
    "                for low,up in zip(freq_limits_low, freq_limits_up)]\n",
    "\n",
    "        power_per_band= np.array(power_per_band)[np.array(power_per_band) > 0]\n",
    "\n",
    "    spectral = - np.sum(power_per_band * np.log2(power_per_band))\n",
    "    return(spectral)\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "def _embed_seq(signal, tau, emb_dim):\n",
    "\n",
    "    N =len(signal)\n",
    "\n",
    "    if emb_dim * tau > N:\n",
    "        raise ValueError(\"Cannot build such a matrix, because D * Tau > N\")\n",
    "\n",
    "    if tau<1:\n",
    "        raise ValueError(\"Tau has to be at least 1\")\n",
    "\n",
    "\n",
    "    Y=np.zeros((emb_dim, N - (emb_dim - 1) * tau))\n",
    "\n",
    "    for i in range(emb_dim):\n",
    "        Y[i] = signal[i *tau : i*tau + Y.shape[1] ]\n",
    "\n",
    "    return(Y.T)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "def complexity_entropy_svd(signal, tau=1, emb_dim=2):\n",
    "    \"\"\"\n",
    "    Computes the Singular Value Decomposition (SVD) entropy of a signal. Based on the `pyrem <https://github.com/gilestrolab/pyrem>`_ repo by Quentin Geissmann.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : list or array\n",
    "        List or array of values.\n",
    "    tau : int\n",
    "        The delay\n",
    "    emb_dim : int\n",
    "        The embedding dimension (*m*, the length of vectors to compare).\n",
    "    Returns\n",
    "    ----------\n",
    "    entropy_svd : float\n",
    "        The SVD entropy as float value.\n",
    "    Example\n",
    "    ----------\n",
    "    >>> import neurokit as nk\n",
    "    >>>\n",
    "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
    "    >>> entropy_svd = nk.complexity_entropy_svd(signal, 1, 2)\n",
    "    Notes\n",
    "    ----------\n",
    "    *Details*\n",
    "    - **SVD Entropy**: Indicator of how many vectors are needed for an adequate explanation of the data set. Measures feature-richness in the sense that the higher the entropy of the set of SVD weights, the more orthogonal vectors are required to adequately explain it.\n",
    "    *Authors*\n",
    "    - Quentin Geissmann (https://github.com/qgeissmann)\n",
    "    *Dependencies*\n",
    "    - numpy\n",
    "    *See Also*\n",
    "    - pyrem package: https://github.com/gilestrolab/pyrem\n",
    "    \"\"\"\n",
    "    mat =  _embed_seq(signal, tau, emb_dim)\n",
    "    W = np.linalg.svd(mat, compute_uv = False)\n",
    "    W /= sum(W) # normalize singular values\n",
    "    entropy_svd = -1*sum(W * np.log2(W))\n",
    "    return(entropy_svd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "def complexity_fd_petrosian(signal):\n",
    "    \"\"\"\n",
    "    Computes the Petrosian Fractal Dimension of a signal. Based on the `pyrem <https://github.com/gilestrolab/pyrem>`_ repo by Quentin Geissmann.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : list or array\n",
    "        List or array of values.\n",
    "    Returns\n",
    "    ----------\n",
    "    fd_petrosian : float\n",
    "        The Petrosian FD as float value.\n",
    "    Example\n",
    "    ----------\n",
    "    >>> import neurokit as nk\n",
    "    >>>\n",
    "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
    "    >>> fd_petrosian = nk.complexity_fd_petrosian(signal, 1, 2)\n",
    "    Notes\n",
    "    ----------\n",
    "    *Details*\n",
    "    - **Petrosian Fractal Dimension**: Provide a fast computation of the FD of a signal by translating the series into a binary sequence.\n",
    "    *Authors*\n",
    "    - Quentin Geissmann (https://github.com/qgeissmann)\n",
    "    *Dependencies*\n",
    "    - numpy\n",
    "    *See Also*\n",
    "    - pyrem package: https://github.com/gilestrolab/pyrem\n",
    "    \"\"\"\n",
    "    diff = np.diff(signal)\n",
    "    # x[i] * x[i-1] for i in t0 -> tmax\n",
    "    prod = diff[1:-1] * diff[0:-2]\n",
    "\n",
    "    # Number of sign changes in derivative of the signal\n",
    "    N_delta = np.sum(prod < 0)\n",
    "    n = len(signal)\n",
    "    fd_petrosian = np.log(n)/(np.log(n)+np.log(n/(n+0.4*N_delta)))\n",
    "    return(fd_petrosian)\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "def complexity_fisher_info(signal, tau=1, emb_dim=2):\n",
    "    \"\"\"\n",
    "    Computes the Fisher information of a signal. Based on the `pyrem <https://github.com/gilestrolab/pyrem>`_ repo by Quentin Geissmann.\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : list or array\n",
    "        List or array of values.\n",
    "    tau : int\n",
    "        The delay\n",
    "    emb_dim : int\n",
    "        The embedding dimension (*m*, the length of vectors to compare).\n",
    "    Returns\n",
    "    ----------\n",
    "    fisher_info : float\n",
    "        The Fisher information as float value.\n",
    "    Example\n",
    "    ----------\n",
    "    >>> import neurokit as nk\n",
    "    >>>\n",
    "    >>> signal = np.sin(np.log(np.random.sample(666)))\n",
    "    >>> fisher_info = nk.complexity_fisher_info(signal, 1, 2)\n",
    "    Notes\n",
    "    ----------\n",
    "    *Details*\n",
    "    - **Fisher Information**:  A way of measuring the amount of information that an observable random variable X carries about an unknown parameter θ of a distribution that models X. Formally, it is the variance of the score, or the expected value of the observed information.\n",
    "    *Authors*\n",
    "    - Quentin Geissmann (https://github.com/qgeissmann)\n",
    "    *Dependencies*\n",
    "    - numpy\n",
    "    *See Also*\n",
    "    - pyrem package: https://github.com/gilestrolab/pyrem\n",
    "    \"\"\"\n",
    "\n",
    "    mat =  _embed_seq(signal, tau, emb_dim)\n",
    "    W = np.linalg.svd(mat, compute_uv = False)\n",
    "    W /= sum(W) # normalize singular values\n",
    "    FI_v = (W[1:] - W[:-1]) **2 / W[:-1]\n",
    "\n",
    "    fisher_info = np.sum(FI_v)\n",
    "    return(fisher_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
